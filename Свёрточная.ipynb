{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/k5Qt9ybwwzGBbQiGbWdr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VadushaLyapushin/rzhavchina/blob/main/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX3o2_Om69K9",
        "outputId": "f16c2a8d-83a2-4b04-f9e0-1ce18b5fb609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rzhavchina'...\n",
            "remote: Enumerating objects: 417, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 417 (delta 4), reused 165 (delta 3), pack-reused 249\u001b[K\n",
            "Receiving objects: 100% (417/417), 76.24 MiB | 28.34 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/VadushaLyapushin/rzhavchina.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "Tqz39YX47JXO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matplotlib.use(\"Agg\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "qkHXxn2_7PFR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "from skimage import exposure\n",
        "import random"
      ],
      "metadata": {
        "id": "_ZqGzhyN7QyM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ImagePathsWithoot = list(paths.list_images(\"/content/rzhavchina/Transform Images/without\"))\n",
        "random.seed(4)\n",
        "print(len(ImagePathsWithoot))\n",
        "ImagePathsSurface = list(paths.list_images(\"/content/rzhavchina/Transform Images/surface\"))\n",
        "random.seed(4)\n",
        "print(len(ImagePathsSurface))\n",
        "ImagePathsDeep = list(paths.list_images(\"/content/rzhavchina/Transform Images/deep\"))\n",
        "random.seed(4)\n",
        "print(len(ImagePathsDeep))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQQnBybj7cqo",
        "outputId": "7e435939-4df2-4bad-e302-c589021c065e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n",
            "49\n",
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ImagePaths = ImagePathsWithoot + ImagePathsSurface + ImagePathsDeep\n",
        "random.shuffle(ImagePaths)\n",
        "print(len(ImagePaths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D5AqNg98A1D",
        "outputId": "708a4234-d83f-43cc-9adf-0303530914c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "labels = []\n",
        "\n",
        "i=0\n",
        "for imagepath in ImagePaths:\n",
        "  image = cv2.imread(imagepath)\n",
        "  image = cv2.resize(image, (32, 32))\n",
        "  data.append(image)\n",
        "  label = imagepath.split(os.path.sep)[-2]\n",
        "  if label == \"without\":\n",
        "    label = [1, 0, 0]\n",
        "  elif label == \"surface\":\n",
        "    label = [0, 1, 0]\n",
        "  elif label == \"deep\":\n",
        "    label = [0, 0, 1]\n",
        "  labels.append(label)\n",
        "\n",
        "print(labels)\n",
        "\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "with open(\"/content/rzhavchina/training/data.pickle\", 'wb') as f:\n",
        "  pickle.dump(data, f)\n",
        "print(\"Data seved\")\n",
        "\n",
        "with open(\"/content/rzhavchina/training/labels.pickle\", 'wb') as f:\n",
        "  pickle.dump(labels, f)\n",
        "print(\"Labels seved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woFPGnaW8GyM",
        "outputId": "dd7c884a-2d7f-4b93-9fe3-76dea51add14"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0]]\n",
            "Data seved\n",
            "Labels seved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.prod(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MOJZ8jx9Y5B",
        "outputId": "ef9ee8b0-d8e6-40e9-a202-cbe08f31b3de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "441"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/rzhavchina/training/data.pickle\", 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "print(\"Data loaded\")\n",
        "\n",
        "with open(\"/content/rzhavchina/training/labels.pickle\", 'rb') as f:\n",
        "  labels = pickle.load(f)\n",
        "print(\"Labels loaded\")\n",
        "\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.15,\n",
        "                                                  random_state=42)\n",
        "\n",
        "print(\"Dataset prepared\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw7VLwC89xxL",
        "outputId": "d2ede5e3-c052-4833-e706-4f54a8ecd07d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded\n",
            "Labels loaded\n",
            "Dataset prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "inputShape = (32, 32, 3)\n",
        "chanDim = -1\n",
        "    # CONV => RELU => BN => POOL\n",
        "model.add(Conv2D(8, (5, 5), padding=\"same\",\n",
        "  input_shape=inputShape))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # first set of (CONV => RELU => CONV => RELU) * 2 => POOL\n",
        "model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# second set of (CONV => RELU => CONV => RELU) * 2 => POOL\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # first set of FC => RELU layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "# second set of FC => RELU layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "# softmax classifier\n",
        "model.add(Dense(3))\n",
        "model.add(Activation(\"softmax\"))\n",
        "# return the constructed network architecture\n",
        "print (\"End\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHC1R6Bt-Hm5",
        "outputId": "fbcb7225-488a-4cee-c41a-011ceadb4ed3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "from skimage import transform\n",
        "from skimage import exposure\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "caKD9JgJ-sGe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/rzhavchina/training/data.pickle\", 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "print(\"Data loaded\")\n",
        "\n",
        "with open(\"/content/rzhavchina/training/labels.pickle\", 'rb') as f:\n",
        "  labels = pickle.load(f)\n",
        "print(\"Labels loaded\")\n",
        "\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.15,\n",
        "                                                  random_state=42)\n",
        "\n",
        "print(\"Dataset prepared\")\n",
        "\n",
        "\n",
        "# classTotals = trainY.sum(axis=0)\n",
        "# classWeight = classTotals.max() / classTotals\n",
        "# print(type(classWeight))\n",
        "\n",
        "classTotals = trainY.sum(axis=0)\n",
        "print(classTotals)\n",
        "classWeight = classTotals.max() / classTotals\n",
        "print(classWeight/1)\n",
        "\n",
        "TestclassTotals = testY.sum(axis=0)\n",
        "TestclassWeight = TestclassTotals.max() / TestclassTotals\n",
        "print(TestclassTotals)\n",
        "print(\"TestclassWeight\")\n",
        "print(TestclassWeight//1)\n",
        "f=TestclassTotals.argmin()\n",
        "\n",
        "\n",
        "\n",
        "# initialize the number of epochs to train for, base learning rate,\n",
        "# and batch size\n",
        "NUM_EPOCHS = 30\n",
        "INIT_LR = 1e-3\n",
        "BS = 32\n",
        "# load the label names\n",
        "target_names= [\"without\", \"surface\", \"deep\"]\n",
        "numLabels = len(target_names)\n",
        "\n",
        "# construct the image generator for data augmentation\n",
        "opt = 'adam'\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# opt = keras.optimizers.legacy.Adam(lr=INIT_LR, decay=INIT_LR / (NUM_EPOCHS * 0.5))\n",
        "\n",
        "# opt = tf.keras.optimizers.legacy.Adam(\n",
        "#     learning_rate=INIT_LR,\n",
        "#     beta_1=0.9,\n",
        "#     beta_2=0.999,\n",
        "#     epsilon=1e-07,\n",
        "#     amsgrad=False,\n",
        "#     name='Adam',\n",
        "#     **kwargs)\n",
        "\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4ZOAFBi-M32",
        "outputId": "9f39cd12-2d7e-4276-d939-9cb4a358e355"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded\n",
            "Labels loaded\n",
            "Dataset prepared\n",
            "[42 41 41]\n",
            "[1.         1.02439024 1.02439024]\n",
            "[7 8 8]\n",
            "TestclassWeight\n",
            "[1. 1. 1.]\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 8)         608       \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 32, 32, 8)         0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 32, 32, 8)         32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 8)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 16)        1168      \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 16, 16, 16)        0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 16, 16, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 16)        2320      \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 16, 16, 16)        0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 16, 16, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 8, 8, 16)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 32)          4640      \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 8, 8, 32)          128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 8, 8, 32)          9248      \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 8, 8, 32)          128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 4, 4, 32)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 128)               0         \n",
            "                                                                 \n",
            " batch_normalization_12 (Ba  (None, 128)               512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 128)               0         \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 128)               512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101987 (398.39 KB)\n",
            "Trainable params: 101267 (395.57 KB)\n",
            "Non-trainable params: 720 (2.81 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug = ImageDataGenerator(\n",
        "\trotation_range=5,\n",
        "\tzoom_range=0.05,\n",
        "\twidth_shift_range=0.05,\n",
        "\theight_shift_range=0.05,\n",
        "\tshear_range=0.10,\n",
        "\thorizontal_flip=False,\n",
        "\tvertical_flip=False,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "print(classWeight)\n",
        "\n",
        "class_Weight = {0: 1.,\n",
        "                1: 1.,\n",
        "                2: 1.}\n",
        "\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='/content/rzhavchina/training/Best_Sign_Class.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tvalidation_data=(testX, testY),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tsteps_per_epoch=trainX.shape[0] // BS,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tepochs=NUM_EPOCHS,\n",
        "                        shuffle=True,\n",
        "                        class_weight=class_Weight,\n",
        "                        callbacks=[checkpointer])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0mTTmdx_UbJ",
        "outputId": "8a57541a-0346-44b9-a184-fbe04b8489cb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network...\n",
            "[1.         1.02439024 1.02439024]\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-5aa1e260e7a8>:22: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - ETA: 0s - loss: 0.8027 - accuracy: 0.8043\n",
            "Epoch 1: val_loss improved from inf to 3.00274, saving model to /content/rzhavchina/training/Best_Sign_Class.h5\n",
            "3/3 [==============================] - 4s 278ms/step - loss: 0.8027 - accuracy: 0.8043 - val_loss: 3.0027 - val_accuracy: 0.2609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.7946 - accuracy: 0.8125\n",
            "Epoch 2: val_loss improved from 3.00274 to 2.69373, saving model to /content/rzhavchina/training/Best_Sign_Class.h5\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 0.8649 - accuracy: 0.8152 - val_loss: 2.6937 - val_accuracy: 0.3478\n",
            "Epoch 3/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.6547 - accuracy: 0.8333\n",
            "Epoch 3: val_loss did not improve from 2.69373\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.5444 - accuracy: 0.8587 - val_loss: 2.7647 - val_accuracy: 0.3043\n",
            "Epoch 4/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3232 - accuracy: 0.8854\n",
            "Epoch 4: val_loss did not improve from 2.69373\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3232 - accuracy: 0.8854 - val_loss: 2.8429 - val_accuracy: 0.4348\n",
            "Epoch 5/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3420 - accuracy: 0.8750\n",
            "Epoch 5: val_loss did not improve from 2.69373\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3827 - accuracy: 0.8750 - val_loss: 2.8070 - val_accuracy: 0.4348\n",
            "Epoch 6/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3141 - accuracy: 0.9062\n",
            "Epoch 6: val_loss improved from 2.69373 to 2.61830, saving model to /content/rzhavchina/training/Best_Sign_Class.h5\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3662 - accuracy: 0.8750 - val_loss: 2.6183 - val_accuracy: 0.4348\n",
            "Epoch 7/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.7082 - accuracy: 0.8125\n",
            "Epoch 7: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.6884 - accuracy: 0.8152 - val_loss: 2.7134 - val_accuracy: 0.3913\n",
            "Epoch 8/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3719 - accuracy: 0.8750\n",
            "Epoch 8: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3118 - accuracy: 0.8958 - val_loss: 2.9394 - val_accuracy: 0.4783\n",
            "Epoch 9/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2157 - accuracy: 0.9062\n",
            "Epoch 9: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2451 - accuracy: 0.9022 - val_loss: 3.1401 - val_accuracy: 0.4783\n",
            "Epoch 10/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8906\n",
            "Epoch 10: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.2991 - accuracy: 0.8913 - val_loss: 3.4915 - val_accuracy: 0.4348\n",
            "Epoch 11/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1881 - accuracy: 0.9333\n",
            "Epoch 11: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.2090 - accuracy: 0.9348 - val_loss: 3.6135 - val_accuracy: 0.4348\n",
            "Epoch 12/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5643 - accuracy: 0.8594\n",
            "Epoch 12: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4836 - accuracy: 0.8696 - val_loss: 3.5111 - val_accuracy: 0.3913\n",
            "Epoch 13/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4708 - accuracy: 0.9062\n",
            "Epoch 13: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.5360 - accuracy: 0.9130 - val_loss: 3.2213 - val_accuracy: 0.4348\n",
            "Epoch 14/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2725 - accuracy: 0.8906\n",
            "Epoch 14: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2740 - accuracy: 0.8804 - val_loss: 3.0096 - val_accuracy: 0.3913\n",
            "Epoch 15/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3543 - accuracy: 0.8958\n",
            "Epoch 15: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3543 - accuracy: 0.8958 - val_loss: 3.1388 - val_accuracy: 0.3043\n",
            "Epoch 16/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1409 - accuracy: 0.9531\n",
            "Epoch 16: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1887 - accuracy: 0.9375 - val_loss: 3.4717 - val_accuracy: 0.2174\n",
            "Epoch 17/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5542 - accuracy: 0.8594\n",
            "Epoch 17: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4420 - accuracy: 0.8958 - val_loss: 3.6546 - val_accuracy: 0.1739\n",
            "Epoch 18/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3650 - accuracy: 0.8478\n",
            "Epoch 18: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3650 - accuracy: 0.8478 - val_loss: 3.5961 - val_accuracy: 0.2609\n",
            "Epoch 19/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3638 - accuracy: 0.8913\n",
            "Epoch 19: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3638 - accuracy: 0.8913 - val_loss: 3.5256 - val_accuracy: 0.2609\n",
            "Epoch 20/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9457\n",
            "Epoch 20: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1223 - accuracy: 0.9457 - val_loss: 3.2854 - val_accuracy: 0.2609\n",
            "Epoch 21/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9896\n",
            "Epoch 21: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0797 - accuracy: 0.9896 - val_loss: 3.1118 - val_accuracy: 0.2609\n",
            "Epoch 22/30\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0701 - accuracy: 0.9667\n",
            "Epoch 22: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2333 - accuracy: 0.9348 - val_loss: 3.1048 - val_accuracy: 0.2609\n",
            "Epoch 23/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9457\n",
            "Epoch 23: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1347 - accuracy: 0.9457 - val_loss: 3.0064 - val_accuracy: 0.2609\n",
            "Epoch 24/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9022\n",
            "Epoch 24: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 1s 178ms/step - loss: 0.2532 - accuracy: 0.9022 - val_loss: 2.8939 - val_accuracy: 0.2609\n",
            "Epoch 25/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9062\n",
            "Epoch 25: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.2876 - accuracy: 0.9062 - val_loss: 2.8202 - val_accuracy: 0.3043\n",
            "Epoch 26/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9583\n",
            "Epoch 26: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1292 - accuracy: 0.9583 - val_loss: 2.8269 - val_accuracy: 0.3478\n",
            "Epoch 27/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9457\n",
            "Epoch 27: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 132ms/step - loss: 0.1675 - accuracy: 0.9457 - val_loss: 2.7840 - val_accuracy: 0.3913\n",
            "Epoch 28/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9674\n",
            "Epoch 28: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0922 - accuracy: 0.9674 - val_loss: 2.8382 - val_accuracy: 0.3913\n",
            "Epoch 29/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9479\n",
            "Epoch 29: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1261 - accuracy: 0.9479 - val_loss: 2.8827 - val_accuracy: 0.2609\n",
            "Epoch 30/30\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9457\n",
            "Epoch 30: val_loss did not improve from 2.61830\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.1362 - accuracy: 0.9457 - val_loss: 2.9139 - val_accuracy: 0.2174\n"
          ]
        }
      ]
    }
  ]
}